{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8a2c20-ac5b-43e1-a94f-1a4ce0fed80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pylab inline\n",
    "import numpy\n",
    "np = numpy\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "plt = pyplot\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "\n",
    "import asyncio\n",
    "\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import collections\n",
    "import operator\n",
    "import itertools\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import json\n",
    "import hjson\n",
    "\n",
    "amap = lambda *args, **kwargs: np.array(list(map(*args)), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdeac72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M3 Max) - 98303 MiB free\n",
      "llama_model_loader: additional 1 GGUFs metadata loaded.\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 724 tensors from /Users/abra/.cache/lm-studio/models/lmstudio-community/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q8_0-00001-of-00002.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.3 70B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.3\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 70B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.3\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Llama 3.1 70B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,5]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,8]       = [\"en\", \"fr\", \"it\", \"pt\", \"hi\", \"es\", ...\n",
      "llama_model_loader: - kv  13:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv  14:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  21:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  22:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  23:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  24:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  25:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                                   split.no u16              = 0\n",
      "llama_model_loader: - kv  37:                                split.count u16              = 2\n",
      "llama_model_loader: - kv  38:                        split.tensors.count i32              = 724\n",
      "llama_model_loader: - type  f32:  162 tensors\n",
      "llama_model_loader: - type q8_0:  562 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 69.82 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.3 70B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size = 38069.89 MiB, (38069.97 / 98304.00)\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size = 33424.42 MiB, (71494.39 / 98304.00)\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  1064.62 MiB\n",
      "llm_load_tensors: Metal_Mapped model buffer size = 38069.89 MiB\n",
      "llm_load_tensors: Metal_Mapped model buffer size = 33424.41 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 4096\n",
      "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x127309190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x1307ab5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x12740a320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1307ace90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1307abf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x12740ad60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x127286600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x127286bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x1057941a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x101c078e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x12740b440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x123b0e960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x101c06d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x130287600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x123b150a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x101c0a6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x101c0b060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x127287070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x130288450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x127309fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1307ad7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x130287ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x127308d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x1307aebf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1217707f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1217703e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x130289eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1307afe90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x123a09c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12740cc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x13028a0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x123a0a850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1272872a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12176fb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1272874d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127287700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121772170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12740d940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x130032450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127287930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1307adfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101c0b290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1307af460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12740bf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12730aea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127287b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127287d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1273084d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101c0c680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121772980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127288340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x101c0bcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x12730a710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x130031150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x130031480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123b06640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x130289640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121770d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13028abf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101c0ca50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101c0d2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101c0d4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101c0e380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12740e180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x130031cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13028b420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13028c7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13028cb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13028bbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1217736a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121773bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101c0f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101c0ee10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105795050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13028d120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1057965f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12730c9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12730d230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121773df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121774fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105796cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105797400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1057956f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12740e8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12740fc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1057985f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1274105a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127410e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101c0f890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101c0fb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1300327d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12740ec10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1300334b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121775880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127288730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1300344d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101c10c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13028d930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123a0aa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123a0bf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123b16ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123b17980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x130033bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123a0c3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13028e460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x130034980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123a0d7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13028f300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13028f6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123c0af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123a0d410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123c0a870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1300351f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101c118f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x130035e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1302901f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1217760c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123a0de90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123a0ec10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123c0ca80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x130290420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123a0f250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105798820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x130291490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127288b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101c10f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1307b1a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1307b2b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1274124e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127411a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127412a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127413070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127288d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1307b2650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1307b2880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1274132a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x130036a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101c0e780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12730e230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127289140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1307b47a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1272896f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101c124d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127289ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x127289ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x1217767f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x121776ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x123c0d060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x121777500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x127414ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1307b3ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127415140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x130034bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1307b4fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1307b5560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x127415cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12728a2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127416630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127417530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123b08f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x130290d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12730fb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1274186a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127310410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101c13680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x130038340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101c13a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123c0d290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121779a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12177a380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12730ef50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12730f180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127310890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127311b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12728aa30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12728b030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12728b260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x130291880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127417b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12728be30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12728c710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1274199a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12728d060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127204080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127312990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127313230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x130036f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x130292df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x130291c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127418b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101c13cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x130037150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127418fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1273150f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1307b6c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x130291e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127204970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1302934e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x130039420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x130294510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x130294e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x130037ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105799f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12741b1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105799770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12741a270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127205290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1273149b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101c13ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x130293930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1307b7850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1307b72d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12741a900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127205920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127206200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12741b830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105799290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x12741c6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x12741c8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x1307b8050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x127206ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x1272076a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1272078d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x130295530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1307b93d0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1280.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   584.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | \n",
      "Model metadata: {'split.count': '2', 'split.no': '0', 'split.tensors.count': '724', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.padding_token_id': '128004', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '8192', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.size_label': '70B', 'general.base_model.0.name': 'Llama 3.1 70B', 'llama.block_count': '80', 'general.base_model.0.organization': 'Meta Llama', 'llama.attention.value_length': '128', 'llama.attention.head_count': '64', 'llama.attention.key_length': '128', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.3', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Llama-3.1-70B', 'llama.context_length': '131072', 'general.file_type': '7', 'general.finetune': 'Instruct', 'llama.feed_forward_length': '28672', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'Llama-3.3', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'general.type': 'model', 'general.name': 'Llama 3.3 70B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama, LlamaGrammar\n",
    "from llama_cpp.llama_speculative import LlamaDraftModel, LlamaPromptLookupDecoding\n",
    "\n",
    "llm = Llama(\n",
    "    model_path='/Users/abra/.cache/lm-studio/models/lmstudio-community/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q8_0-00001-of-00002.gguf',\n",
    "    # model_path='/Users/abra/.cache/lm-studio/models/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf',\n",
    "    # model_path='/Users/abra/.cache/lm-studio/models/lmstudio-community/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q4_K_M.gguf',\n",
    "    # model_path='/Users/abra/.cache/lm-studio/models/lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF/Meta-Llama-3.1-70B-Instruct-Q8_0-00001-of-00002.gguf',\n",
    "    logits_all=True,\n",
    "    n_ctx=1024 * 4,\n",
    "    n_gpu_layers=-1,\n",
    "    # draft_model=LlamaPromptLookupDecoding(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7253a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_BEGIN_OF_TEXT = llm.tokenize(b'<|begin_of_text|>', add_bos=False, special=True)\n",
    "TOKEN_END_OF_TURN = llm.tokenize(b'<|eot_id|>', add_bos=False, special=True)\n",
    "TOKEN_END_OF_MESSAGE = llm.tokenize(b'<|eom_id|>', add_bos=False, special=True)\n",
    "TOKEN_START_TURN_HEADER = llm.tokenize(b'<|start_header_id|>', add_bos=False, special=True)\n",
    "TOKEN_END_TURN_HEADER = llm.tokenize(b'<|end_header_id|>', add_bos=False, special=True)\n",
    "\n",
    "def str_to_tokens(s):\n",
    "    return llm.tokenize(\n",
    "        s.encode(), \n",
    "        add_bos=False, \n",
    "        special=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_input_ids = None\n",
    "last_scores = None\n",
    "\n",
    "def global_logging_logit_processor(\n",
    "    input_ids: np.typing.NDArray[np.intc], scores: np.typing.NDArray[np.single],\n",
    ") -> np.typing.NDArray[np.single]:\n",
    "    global last_input_ids, last_scores\n",
    "    last_input_ids = np.array(input_ids)\n",
    "    last_scores = np.array(scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "json_schema_str = '''\n",
    "\n",
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"german_saying\": { \"type\": \"string\" },\n",
    "#    \"literal_english_translation\": { \"type\": \"string\" },\n",
    "#    \"meaning\": { \"type\": \"string\" },\n",
    "  },\n",
    "  \"required\": [\n",
    "    \"german_saying\",\n",
    "#    \"literal_english_translation\",\n",
    "#    \"meaning\",\n",
    "  ]\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "json_schema_str = json.dumps(hjson.loads(json_schema_str))\n",
    "\n",
    "tokens = (\n",
    "    TOKEN_BEGIN_OF_TEXT + \n",
    "\n",
    "    TOKEN_START_TURN_HEADER + str_to_tokens('system') + TOKEN_END_TURN_HEADER + str_to_tokens('\\n\\n') +\n",
    "    str_to_tokens('''\n",
    "Reply STRICTLY conforming to this JSON schema.\n",
    "    '''.strip()) +\n",
    "    str_to_tokens('\\n') +\n",
    "    str_to_tokens(json_schema_str) + \n",
    "    str_to_tokens('\\n') +\n",
    "    str_to_tokens('''\n",
    "Do NOT output anything else besides a single JSON object.\n",
    "    '''.strip()) +\n",
    "    TOKEN_END_OF_TURN + \n",
    "    \n",
    "    TOKEN_START_TURN_HEADER + str_to_tokens('user') + TOKEN_END_TURN_HEADER + str_to_tokens('\\n\\n') +\n",
    "    str_to_tokens('''\n",
    "Tell me a German saying.\n",
    "    '''.strip()) +\n",
    "    TOKEN_END_OF_TURN + \n",
    "\n",
    "    TOKEN_START_TURN_HEADER + str_to_tokens('assistant') + TOKEN_END_TURN_HEADER + str_to_tokens('\\n\\n') +\n",
    "    str_to_tokens('''\n",
    "{\"german_saying\": \"\n",
    "    '''.strip())\n",
    ")\n",
    "\n",
    "# # llm.reset()\n",
    "# llm.set_seed(0)\n",
    "# generator = llm.generate(\n",
    "#     tokens=tokens,\n",
    "#     logits_processor=[],\n",
    "#     # grammar=LlamaGrammar.from_json_schema(json_schema_str),\n",
    "# )\n",
    "\n",
    "# print(llm.detokenize(tokens, prev_tokens=None, special=True).decode(), end='')\n",
    "\n",
    "# shown_tokens = list(tokens)\n",
    "# unshown_tokens = []\n",
    "\n",
    "# for new_token in generator:\n",
    "#     if new_token is None or new_token == TOKEN_END_OF_TURN[0]:\n",
    "#         break\n",
    "\n",
    "#     unshown_tokens.append(new_token)\n",
    "\n",
    "#     bytes_to_show = llm.detokenize(unshown_tokens, prev_tokens=shown_tokens, special=True)\n",
    "\n",
    "#     try:\n",
    "#         string_to_show = bytes_to_show.decode()\n",
    "#     except UnicodeDecodeError as e:\n",
    "#         # spanish inverted question mark\n",
    "#         print('¿', end='')\n",
    "#         continue\n",
    "    \n",
    "#     shown_tokens.extend(unshown_tokens)\n",
    "#     unshown_tokens = []\n",
    "    \n",
    "#     print(string_to_show, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = llm.generate(\n",
    "    tokens=tokens,\n",
    "    logits_processor=[global_logging_logit_processor],\n",
    "    # grammar=LlamaGrammar.from_json_schema(json_schema_str),\n",
    ")\n",
    "next_token = next(generator)\n",
    "llm.detokenize([next_token], prev_tokens=tokens, special=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c62cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.reset()\n",
    "llm.eval(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6beac60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 68941001\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 68941001\n",
      "Llama.save_state: saving 68941001 bytes of llama state\n"
     ]
    }
   ],
   "source": [
    "prompt_fed_state = llm.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80ec8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sortedcontainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce5e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class State:\n",
    "    acc_probability: float\n",
    "    new_tokens: list[int]\n",
    "    llm_state_before_last_token: typing.Any\n",
    "    llm_state_after_last_token: typing.Any\n",
    "\n",
    "    def __lt__(self, o):\n",
    "        return self.acc_probability < o.acc_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89e35f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_states = sortedcontainers.SortedList()\n",
    "\n",
    "states = sortedcontainers.SortedList([\n",
    "    State(1.0, [], None, prompt_fed_state),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9842729f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e978aa87a14c10b2865b8cdc05c1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34922649\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34922649\n",
      "Llama.save_state: saving 34922649 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 35250341\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 35250341\n",
      "Llama.save_state: saving 35250341 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34922649\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34922649\n",
      "Llama.save_state: saving 34922649 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34922649\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34922649\n",
      "Llama.save_state: saving 34922649 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 35250341\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 35250341\n",
      "Llama.save_state: saving 35250341 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 35578033\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 35578033\n",
      "Llama.save_state: saving 35578033 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34922649\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34922649\n",
      "Llama.save_state: saving 34922649 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34922649\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34922649\n",
      "Llama.save_state: saving 34922649 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 35250341\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 35250341\n",
      "Llama.save_state: saving 35250341 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 35578033\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 35578033\n",
      "Llama.save_state: saving 35578033 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 35905725\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 35905725\n",
      "Llama.save_state: saving 35905725 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 36233417\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 36233417\n",
      "Llama.save_state: saving 36233417 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 36561109\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 36561109\n",
      "Llama.save_state: saving 36561109 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29024193\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29024193\n",
      "Llama.save_state: saving 29024193 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 27713425\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 27713425\n",
      "Llama.save_state: saving 27713425 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31645729\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31645729\n",
      "Llama.save_state: saving 31645729 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34922649\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34922649\n",
      "Llama.save_state: saving 34922649 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31973421\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31973421\n",
      "Llama.save_state: saving 31973421 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32301113\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32301113\n",
      "Llama.save_state: saving 32301113 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28368809\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28368809\n",
      "Llama.save_state: saving 28368809 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28041117\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28041117\n",
      "Llama.save_state: saving 28041117 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29679577\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29679577\n",
      "Llama.save_state: saving 29679577 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30007269\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30007269\n",
      "Llama.save_state: saving 30007269 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 28696501\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 28696501\n",
      "Llama.save_state: saving 28696501 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 29351885\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 29351885\n",
      "Llama.save_state: saving 29351885 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30990345\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30990345\n",
      "Llama.save_state: saving 30990345 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32628805\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32628805\n",
      "Llama.save_state: saving 32628805 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 32956497\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 32956497\n",
      "Llama.save_state: saving 32956497 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33284189\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33284189\n",
      "Llama.save_state: saving 33284189 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33611881\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33611881\n",
      "Llama.save_state: saving 33611881 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 33939573\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 33939573\n",
      "Llama.save_state: saving 33939573 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34267265\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34267265\n",
      "Llama.save_state: saving 34267265 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 34594957\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 34594957\n",
      "Llama.save_state: saving 34594957 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30334961\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30334961\n",
      "Llama.save_state: saving 30334961 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 31318037\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 31318037\n",
      "Llama.save_state: saving 31318037 bytes of llama state\n",
      "Llama.save_state: saving llama state\n",
      "Llama.save_state: got state size: 30662653\n",
      "Llama.save_state: allocated state\n",
      "Llama.save_state: copied llama state: 30662653\n",
      "Llama.save_state: saving 30662653 bytes of llama state\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10000)):\n",
    "    state = states.pop(-1)\n",
    "\n",
    "    if state.llm_state_after_last_token is not None:\n",
    "        llm.load_state(state.llm_state_after_last_token)\n",
    "        llm_state = state.llm_state_after_last_token\n",
    "    else:\n",
    "        llm.load_state(state.llm_state_before_last_token)\n",
    "        llm.eval(state.new_tokens[-1:])\n",
    "        llm_state = llm.save_state()\n",
    "    \n",
    "    last_scores = None\n",
    "    next_token = llm.sample(\n",
    "        logits_processor=[global_logging_logit_processor]\n",
    "    )\n",
    "\n",
    "    logits = np.array(last_scores)\n",
    "    logits -= np.max(logits)\n",
    "    exp_logits = np.exp(logits)\n",
    "    probs = exp_logits / np.sum(exp_logits)\n",
    "\n",
    "    top_indices = np.argsort(probs)[-1000:]\n",
    "\n",
    "    for next_token in top_indices:\n",
    "        new_prob = state.acc_probability * probs[next_token]\n",
    "\n",
    "        new_state = State(\n",
    "            acc_probability=new_prob,\n",
    "            new_tokens=state.new_tokens + [next_token],\n",
    "            llm_state_before_last_token=llm_state,\n",
    "            llm_state_after_last_token=None,\n",
    "        )\n",
    "        \n",
    "        if next_token == TOKEN_END_OF_TURN[0]:\n",
    "            finished_states.add(new_state)\n",
    "        else:\n",
    "            states.add(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37a2945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "48.797053% -> Morgenstund hat Gold im Mund\"}\n",
      "38.437265% -> Aller Anfang ist schwer\"}\n",
      "4.872750% -> Morgenstunde hat Gold im Munde\"}\n",
      "0.737206% -> Alle guten Dinge sind drei\"}\n",
      "0.568172% -> Wenn das Leben dir Zitronen gibt, mache Limonade daraus\"}\n",
      "0.372180% -> Aufgeschoben ist nicht aufgehoben\"}\n",
      "0.333183% -> Morgensstund hat Gold im Mund\"}\n",
      "0.292641% -> Wenn das Leben dir Zitronen gibt, mache Limonade\"}\n",
      "0.288285% -> Wenn man nicht wandert, kommt man nicht weiter.\"}\n",
      "0.282875% -> Nach dem Regen kommt die Sonne\"}\n",
      "0.226366% -> Wenn man nicht wandert, kommt man nicht weiter\"}\n",
      "0.119454% -> Ich verstehe nur Bahnhof\"}\n",
      "0.103320% -> Wenn man nicht wandert, kommt man nicht vorwärts\"}\n",
      "0.083417% -> Alle Wege führen nach Rom\"}\n",
      "0.082698% -> Wenn die Katze aus dem Haus ist, tanzen die M\\u00e4use\"}\n",
      "0.076038% -> Wer anderen eine Grube gr\\u00e4bt, f\\u00e4llt selbst hinein\"}\n",
      "0.065530% -> Wenn man nicht wandert, kommt man nicht zum Ziel\"}\n",
      "0.064269% -> Alle guten Dinge sind drei.\"}\n",
      "0.062211% -> Übung macht den Meister\"}\n",
      "0.057014% -> Alle gute Dinge sind drei\"}\n",
      "0.056575% -> Aller Anfang ist leicht\"}\n",
      "0.054584% -> Wer anderen eine Grube grabt, fällt selbst hinein\"}\n",
      "0.053348% -> Aufgeschoben ist nicht aufgehoben.\"}\n",
      "0.047358% -> Wenn man im Glashaus sitzt, sollte man nicht mit Steinen werfen\"}\n",
      "0.046965% -> Einigkeit macht stark\"}\n",
      "0.044313% -> Alles hat ein Ende, nur die Wurst hat zwei.\"}\n",
      "0.043224% -> Wenn das Leben Dir Zitronen gibt, mache Limonade daraus\"}\n",
      "0.041445% -> Klein, aber fein\"}\n",
      "0.040094% -> Vorsicht ist die Mutter der Porzellankiste\"}\n",
      "0.039831% -> Wenn man nicht wandern kann, muss man fliegen\"}\n",
      "0.038138% -> Alles in Ordnung\"}\n",
      "0.036963% -> Wer anderen eine Grube gräbt, fällt selbst hinein\"}\n",
      "0.034413% -> Wenn die Katze aus dem Haus ist, tanzen die Mäuse auf dem Tisch\"}\n",
      "0.033615% -> Morgens klug, abends reich\"}\n",
      "0.031119% -> Wenn man den Finger zeigt, zeigt man auch den Mond\"}\n",
      "0.028965% -> Morgens klug, abends dumm\"}\n",
      "0.028925% -> Wenn man im Glashaus sitzt, sollte man nicht mit Steinen werfen.\"}\n",
      "0.028280% -> Wenn man nicht wandert, kommt man nicht zum Ziel.\"}\n",
      "0.028213% -> Wenn man im Glashaus sitzt, soll man nicht mit Steinen werfen\"}\n",
      "0.027977% -> Wer anderen eine Grube grabt, f\\u00e4llt selbst hinein\"}\n",
      "0.027638% -> Kinder und Narren sagen immer die Wahrheit\"}\n",
      "0.027105% -> Übermut tut selten gut\"}\n",
      "0.025636% -> Wenn das Leben dir Zitronen gibt, mache Limonade daraus.\"}\n",
      "0.024386% -> Wie man in den Wald hineinruft, so schallt es heraus\"}\n",
      "0.024356% -> Aus den Augen, aus dem Sinn\"}\n",
      "0.022908% -> Man soll den Tag nicht vor dem Abend loben\"}\n",
      "0.021934% -> Wenn das Leben dir Zitronen gibt, mach Limonade daraus\"}\n",
      "0.021892% -> Wenn man nicht wandert, kommt man nicht vorwärts.\"}\n",
      "0.020116% -> Aller Anfang ist schwer.\"}\n",
      "0.018835% -> Morgens klug, abends bl\\u00f6d\"}\n",
      "0.017895% -> Wenn man nicht wahnsinnig wird, hat man keine Chance\"}\n",
      "0.017173% -> Kleinvieh macht auch Mist\"}\n",
      "0.016206% -> Kinder und Narren sagen die Wahrheit\"}\n",
      "0.015551% -> Allein geht man schneller, zusammen kommt man weiter\"}\n",
      "0.015327% -> Viele Köche verderben den Brei\"}\n",
      "0.014699% -> Wenn man nicht wandern kann, muss man wandern\"}\n",
      "0.013779% -> Wenn man den Finger zeigt, zeigt man auch auf sich selbst\"}\n",
      "0.013261% -> Wenn man den Wald vor lauter Bäumen nicht sieht\"}\n",
      "0.013080% -> Wenn man den Wald vor lauter Bäumen nicht mehr sieht\"}\n",
      "0.012917% -> Morgens klug, abends blöd\"}\n",
      "0.012904% -> Wenn man den Wald vor lauter B\\u00e4umen nicht mehr sieht\"}\n",
      "0.011722% -> Alle Achtung\"}\n",
      "0.011680% -> Wenn man nicht wandern kann, muss man bleiben, wo man ist.\"}\n",
      "0.011500% -> Wenn der Berg nicht zum Propheten kommt, muss der Prophet zum Berg gehen.\"}\n",
      "0.011397% -> Was du heute kannst besorgen, verschiebe nicht auf morgen\"}\n",
      "0.011364% -> Alle Wege f\\u00fchren nach Rom\"}\n",
      "0.011168% -> Wenn man nicht wandert, kommt man nirgendwo hin\"}\n",
      "0.011028% -> Wenn man im Gl\\u00fcck stirbt, stirbt man nicht\"}\n",
      "0.010977% ->  Alle guten Dinge sind drei.\"}\n",
      "0.010436% -> Wenn man nicht weiß, wohin man will, kommt man sicher nicht an.\"}\n",
      "0.010184% -> Wenn man nicht wandert, kommt man nicht vorw\\u00e4rts\"}\n",
      "0.009889% -> Nach dem Spiel ist vor dem Spiel\"}\n",
      "0.009819% -> Wenn man nicht wandern kann, muss man eben fliegen\"}\n",
      "0.009749% -> Wenn man nicht wandern kann, muss man reiten.\"}\n",
      "0.009581% -> Wenn man nicht wandern kann, muss man reisen.\"}\n",
      "0.009303% -> Wenn das Leben Dir Zitronen gibt, mache Limonade\"}\n",
      "0.008923% -> Eine Hand wäscht die andere\"}\n",
      "0.008530% -> Wenn man den Finger zeigt, zeigt man auch den Mond.\"}\n",
      "0.008465% -> Wenn die Katze aus dem Haus ist, tanzen die M\\u00e4use auf dem Tisch\"}\n",
      "0.008448% -> Wenn man nicht weiß, wohin man will, kommt man nirgendwo an.\"}\n",
      "0.008387% -> Allein geht man schnell, zusammen geht man weit\"}\n",
      "0.008307% -> Wenn man nicht wandert, kommt man nicht voran.\"}\n",
      "0.008288% ->  Aufgeschoben ist nicht aufgehoben.\"}\n",
      "0.008193% -> Vorsicht ist besser als Nachsicht\"}\n",
      "0.007948% -> Wenn man im Glashaus sitzt, soll man nicht mit Steinen werfen.\"}\n",
      "0.007935% -> Wenn man den Wald vor lauter B\\u00e4umen nicht sieht\"}\n",
      "0.007909% -> Morgenstunden haben Gold im Munde\"}\n",
      "0.007776% -> Wenn man nicht wandern kann, muss man reisen\"}\n",
      "0.007759% -> Man soll den Tag nicht vor dem Abend loben.\"}\n",
      "0.007737% -> Wenn man im Gl\\u00fcck hat, hat man alles\"}\n",
      "0.007622% -> Wenn man nicht wandern kann, muss man reiten\"}\n",
      "0.007365% -> Zeit ist Geld\"}\n",
      "0.007341% -> Einbildung ist auch eine Bildung\"}\n",
      "0.007148% -> Über den Wolken ist auch Sonnenschein\"}\n",
      "0.006902% -> Wenn man ins Wasser fällt, muss man schwimmen\"}\n",
      "0.006784% -> Der Apfel f\\u00e4llt nicht weit vom Stamm\"}\n",
      "0.006597% -> Wenn man nicht wandelt, kommt man nicht zum Ziel\"}\n",
      "0.006535% -> Kinder und Betrunkene sagen immer die Wahrheit\"}\n",
      "0.006296% -> Wenn man nicht weiß, wohin man will, kommt man irgendwann an, wo man nicht hin wollte.\"}\n",
      "0.006250% -> Aller Anfang ist schwierig\"}\n",
      "0.006167% -> Morgensstunde hat Gold im Munde\"}\n",
      "0.005947% -> Wenn man nicht wandern kann, muss man fliegen.\"}\n",
      "0.005567% -> Wenn der Berg nicht zum Propheten kommt, muss der Prophet zum Berg kommen.\"}\n",
      "0.005512% -> Wenn man ins Wasser f\\u00e4llt, muss man schwimmen\"}\n",
      "0.005485% -> Allein geht man schnell, zusammen geht man weiter\"}\n",
      "0.005467% ->  Wenn man nicht wandert, kommt man nicht weiter.\"}\n",
      "0.005431% -> Ein Apfel pro Tag h\\u00e4lt den Arzt fern\"}\n",
      "0.005352% -> Wenn man keine Wurzeln hat, kann man nicht fliegen\"}\n",
      "0.005318% -> Klein aber fein\"}\n",
      "0.000000% -> Wenn man nicht wandert, kommt man nicht weiter.\"\n",
      "0.000000% -> Wenn man nicht wandern kann, muss man bleiben, wo man ist.\"\n",
      "0.000000% -> Wenn man nicht wandern kann, muss man reisen.\"\n",
      "0.000000% -> Wenn man nicht wandert, kommt man nicht zum Ziel.\"\n",
      "0.000000% -> Wenn man nicht wandert, kommt man nicht vorwärts.\"\n",
      "0.000000% -> Wenn man nicht wandern kann, muss man reiten.\"\n",
      "0.000000% -> Alle guten Dinge sind drei.\"\n",
      "0.000000% -> Wenn man im Glashaus sitzt, sollte man nicht mit Steinen werfen.\"\n",
      "0.000000% -> Wenn man den Finger zeigt, zeigt man auch den Mond.\"\n",
      "0.000000% -> Alles hat ein Ende, nur die Wurst hat zwei.\"\n",
      "0.000000% -> Wenn man nicht wandern kann, muss man fliegen.\"\n",
      "0.000000% -> Aufgeschoben ist nicht aufgehoben.\"\n",
      "0.000000% -> Aller Anfang ist schwer.\"\n",
      "0.000000% -> Wenn das Leben dir Zitronen gibt, mache Limonade daraus.\"\n",
      "0.000000% -> Wenn man nicht wandert, kommt man nicht voran.\"\n",
      "0.000000% -> Wenn man im Glashaus sitzt, soll man nicht mit Steinen werfen.\"\n",
      "0.000000% -> Wenn man nicht weiß, wohin man will, kommt man irgendwann an, wo man nicht hin wollte.\"\n",
      "0.000000% -> Wenn der Berg nicht zum Propheten kommt, muss der Prophet zum Berg gehen.\"\n",
      "0.000000% -> Wenn man nicht weiß, wohin man will, kommt man nirgendwo an.\"\n",
      "0.000000% -> Wenn man nicht weiß, wohin man will, kommt man sicher nicht an.\"\n",
      "0.000000% -> Man soll den Tag nicht vor dem Abend loben.\"\n",
      "0.000000% -> Wenn der Berg nicht zum Propheten kommt, muss der Prophet zum Berg kommen.\"\n",
      "0.000000% ->  Wenn man nicht wandert, kommt man nicht weiter.\"\n",
      "0.000000% ->  Alle guten Dinge sind drei.\"\n",
      "0.000000% ->  Aufgeschoben ist nicht aufgehoben.\"\n",
      "\n",
      "in progress\n",
      "0.005288% -> Um den heißen Brei her\n",
      "0.005280% -> Wenn man nicht wanzen hat, kann\n",
      "0.005274% -> Wer anderen eine Grube graebt, faellt\n",
      "0.005269% -> Wenn man denkt, es geht\n",
      "0.005267% ->  Nach dem Regen kommt die\n",
      "0.005263% -> Absence makes the heart grow fonder, or as we say in German: \\\"\n",
      "0.005253% -> Der Appet\n",
      "0.005252% -> Wenn man trotzdem sagt, dann sagt man trotz\n",
      "0.005234% -> Wenn man nicht wa\n",
      "0.005225% -> Einigkeit und\n"
     ]
    }
   ],
   "source": [
    "print('finished')\n",
    "for i in finished_states[::-1]:\n",
    "    print(f'{i.acc_probability:%}', '->', llm.detokenize(i.new_tokens[:-1], prev_tokens=None, special=True).decode())\n",
    "\n",
    "print()\n",
    "print('in progress')\n",
    "\n",
    "for i in states[-10:][::-1]:\n",
    "    print(f'{i.acc_probability:%}', '->', llm.detokenize(i.new_tokens, prev_tokens=None, special=True).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f35b7c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W LATIN CAPITAL LETTER W\n",
      "e LATIN SMALL LETTER E\n",
      "n LATIN SMALL LETTER N\n",
      "n LATIN SMALL LETTER N\n",
      "  SPACE\n",
      "d LATIN SMALL LETTER D\n",
      "i LATIN SMALL LETTER I\n",
      "e LATIN SMALL LETTER E\n",
      "  SPACE\n",
      "K LATIN CAPITAL LETTER K\n",
      "a LATIN SMALL LETTER A\n",
      "t LATIN SMALL LETTER T\n",
      "z LATIN SMALL LETTER Z\n",
      "e LATIN SMALL LETTER E\n",
      "  SPACE\n",
      "a LATIN SMALL LETTER A\n",
      "u LATIN SMALL LETTER U\n",
      "s LATIN SMALL LETTER S\n",
      "  SPACE\n",
      "d LATIN SMALL LETTER D\n",
      "e LATIN SMALL LETTER E\n",
      "m LATIN SMALL LETTER M\n",
      "  SPACE\n",
      "H LATIN CAPITAL LETTER H\n",
      "a LATIN SMALL LETTER A\n",
      "u LATIN SMALL LETTER U\n",
      "s LATIN SMALL LETTER S\n",
      "  SPACE\n",
      "i LATIN SMALL LETTER I\n",
      "s LATIN SMALL LETTER S\n",
      "t LATIN SMALL LETTER T\n",
      ", COMMA\n",
      "  SPACE\n",
      "t LATIN SMALL LETTER T\n",
      "a LATIN SMALL LETTER A\n",
      "n LATIN SMALL LETTER N\n",
      "z LATIN SMALL LETTER Z\n",
      "e LATIN SMALL LETTER E\n",
      "n LATIN SMALL LETTER N\n",
      "  SPACE\n",
      "d LATIN SMALL LETTER D\n",
      "i LATIN SMALL LETTER I\n",
      "e LATIN SMALL LETTER E\n",
      "  SPACE\n",
      "M LATIN CAPITAL LETTER M\n",
      "\\ REVERSE SOLIDUS\n",
      "u LATIN SMALL LETTER U\n",
      "0 DIGIT ZERO\n",
      "0 DIGIT ZERO\n",
      "e LATIN SMALL LETTER E\n",
      "4 DIGIT FOUR\n",
      "u LATIN SMALL LETTER U\n",
      "s LATIN SMALL LETTER S\n",
      "e LATIN SMALL LETTER E\n",
      "\" QUOTATION MARK\n",
      "} RIGHT CURLY BRACKET\n",
      "< LESS-THAN SIGN\n",
      "| VERTICAL LINE\n",
      "e LATIN SMALL LETTER E\n",
      "o LATIN SMALL LETTER O\n",
      "t LATIN SMALL LETTER T\n",
      "_ LOW LINE\n",
      "i LATIN SMALL LETTER I\n",
      "d LATIN SMALL LETTER D\n",
      "| VERTICAL LINE\n",
      "> GREATER-THAN SIGN\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "for i in s:\n",
    "    print(i, unicodedata.name(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
